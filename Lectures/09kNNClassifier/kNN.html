<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Nearest Neighbour Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Anastasios Panagiotelis" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/mtheme.css" type="text/css" />
    <link rel="stylesheet" href="css/mod.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Nearest Neighbour Classification
## Data Visualisation and Analytics
### Anastasios Panagiotelis
### Lecture 9

---


class: inverse, center, middle

# Love Thy Neighbour



---

# Nearest Neighbours 

- Recall that we discussed nearest neigbours during the topic on LOESS.
--

- Now we will use nearest neighbours in the context of classification.
--

- Illustration with 2 predictors since these are easy to understand.
--

- Data is a subsample of 100 observations from the Credit default data.

---

# Credit Data

&lt;img src="kNN_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

# Default or not?

&lt;img src="kNN_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

---

# Default or not?

&lt;img src="kNN_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

# Basic Idea

- For the first point (limit 350000, age 50) most nearby points in the training data are not defaults (orange).
  - Predict no default
- For the second point (limit 50000, age 45) most nearby points in the training data are defaults (black).
  - Predict default

---

# More formally

- Concentrate on the `\(k\)` nearest points, also known as the k Nearest Neighbours or k-NN.
- Let `\(\mathcal{N}_{\mathbf{x},k}\)` be the set of `\(k\)` training points closest to `\(\mathbf{x}\)`.
- The predicted probability of Default is

`$$\frac{1}{k}\sum\limits_{i\in\mathcal{N}_{\mathbf{x},k}}I(y_i=\mbox{Default})$$`

---

# First example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

# First example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

# Prediction

- The four nearest neighbours are all *Non-defaults*
--

- Therefore the predicted default probability for an individual with limit of 350000 and age 50 is zero.
--

- We predict this individual does not default.

---


# Second example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---

# Second example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

# Prediction

- The four nearest neighbours include three *Defaults* and one *Non-default*
--

- Therefore the predicted default probability for an individual with limit of 50000 and age 45 is 0.75.
--

- We predict this individual does default.

---

# Your Turn

- Consider the case where `\(k=10\)`
- What is the predicted probability of default for the first example (350000 limit 50 years age)?
- What is the predicted probability of default for the second example (50000 limit 45 years age)?

---

# First example (k=10)

&lt;img src="kNN_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---


# Second example (k=10)

&lt;img src="kNN_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

# Answers

- What is the predicted probability of default for the first example (350000 limit 50 years age)?
  - The predicted probability of default is 0.
- What is the predicted probability of default for the second example (50000 limit 45 years age)?
  - The predicted probability of default is 0.6.
  
---
class: inverse, middle, center

# Decision Boundary

---

# Data again

&lt;img src="kNN_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

---

# Decision Boundaries

- For the next few slides a grid of evaluation points has been created.
- If k-NN predicts "Default" the grid point is colored black.
- If k-NN predicts "No default" the grid point is colored yellow.
- Plotting these gives an idea of the *decision boundaries*.

---

# Decision boundary (k=1)

&lt;img src="kNN_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

---

# Decision boundary (k=3)

&lt;img src="kNN_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---


# Decision boundary (k=11)

&lt;img src="kNN_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

---
# Decision boundary (k=17)

&lt;img src="kNN_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---

# Choosing `\(k\)`

- Smaller values of `\(k\)` lead to very complicated decision boundaries.
- Larger values of `\(k\)` provide smoother boundaries although there are still some unstable regions.
- Larger values of `\(k\)` also slow down computation for big datasets.

---

# Choosing `\(k\)`

- A general rule of thumb is to choose `\(k\approx\sqrt{n}\)`
--

- Also it is advised to choose a value of `\(k\)` that is NOT a multiple of the number of classes.
--

- This avoids ties.
--

- When ties are present, a class is chosen randomly.
--

- For the two class problem, choose odd `\(k\)`.

---
# Choosing `\(k\)`

- The value of `\(k\)` can also be chosen by trying many possible values of `\(k\)`.
--

- The optimal value can be selected so that the *test* misclassifcation error rate is minimised.
--

- Another alternative is to use cross validation.

---
class: inverse, middle, center

# Instability of kNN

---

# High variability

- A problem of k-NN classification is that it is highly variable.
- Small changes in the training data change the decision boundaries dramatically.
- This is much more pronounced for smaller values of `\(k\)`.
- The next few slides show different subsamples of training data.

---

# Original Training Sample (k=3)

&lt;img src="kNN_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---


# Different Training Sample (k=3)

&lt;img src="kNN_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

---


# Original Training Sample (k=11)

&lt;img src="kNN_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

---


# Different Training Sample (k=11)

&lt;img src="kNN_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

# An ethical problem

- A classifier with such high variability obviously poses statistical problems.
--

- It also poses ethical issues.
--

- Suppose the algorithm is used to decide who recieves a loan or who is audited by the tax office.
--

- It should be possible to clearly explain to this person why their loan application was rejected or why they were chosen for audit.
--

- Having such a sensitive algorithm makes this difficult.

---
class: inverse, center, middle

# k-NN Classification in R

---

# Package

- The k-NN algorithm can be implemented using the `class` package.
- We will go through an example using the data that has already been cleaned and is in the file *knnexample.RData*
- Load this file using

```r
load('knnexample.RData')
```

---

# Data

- A data frame of 250 randomly selected training observations for the predictors LIMIT_BAL and AGE (`train_x`)
- The values of the target variable for the training data stored as a factor (`train_y`)
- A data frame of 250 randomly selected test observations for the predictors LIMIT_BAL and AGE (`test_x`)
- The values of the target variable for the test data stored as a factor (`test_y`)

---

# Standardisation

- An important issue with k-NN is that distances can be sensitive to units of messurement.
- For this reason we should standardise so that all predictors have a standard deviation of 1. 


```r
train_x_std&lt;-scale(train_x)
mean_train_x&lt;-attr(train_x_std,"scaled:center")
std_train_x&lt;-attr(train_x_std,"scaled:scale")
```

---

# Standardise training data

- The test data should also be standardised but using the same mean and standard deviation


```r
test_x_std&lt;-scale(test_x,
                   center = mean_train_x,
                   scale = std_train_x)
```

- Now the data are ready to be used in the `knn` function

---

# Using knn

With k=1


```r
knn(train_x_std,
    test_x_std,
    train_y,k=1)-&gt;yhat_k1
print(yhat_k1)
```

```
##   [1] No Default Default    Default    Default    No Default No Default
##   [7] No Default No Default No Default Default    No Default No Default
##  [13] No Default No Default No Default Default    No Default No Default
##  [19] No Default No Default Default    No Default Default    Default   
##  [25] No Default No Default Default    No Default Default    No Default
##  [31] Default    No Default No Default No Default No Default No Default
##  [37] Default    No Default No Default Default    Default    No Default
##  [43] No Default No Default No Default No Default Default    No Default
##  [49] No Default No Default Default    No Default No Default No Default
##  [55] No Default No Default No Default Default    No Default No Default
##  [61] Default    No Default No Default Default    No Default No Default
##  [67] Default    Default    No Default No Default No Default No Default
##  [73] No Default No Default No Default Default    No Default No Default
##  [79] No Default No Default No Default No Default No Default No Default
##  [85] Default    No Default No Default No Default No Default No Default
##  [91] No Default No Default No Default No Default Default    No Default
##  [97] No Default No Default No Default No Default No Default No Default
## [103] No Default No Default No Default No Default Default    No Default
## [109] No Default Default    No Default No Default No Default Default   
## [115] No Default No Default No Default No Default No Default No Default
## [121] No Default No Default Default    No Default Default    No Default
## [127] No Default Default    No Default No Default No Default No Default
## [133] No Default No Default No Default No Default No Default No Default
## [139] No Default No Default Default    No Default Default    No Default
## [145] Default    No Default No Default No Default No Default No Default
## [151] No Default No Default No Default Default    No Default Default   
## [157] No Default No Default No Default No Default No Default Default   
## [163] No Default No Default No Default No Default No Default No Default
## [169] No Default No Default No Default No Default No Default No Default
## [175] No Default No Default No Default Default    Default    No Default
## [181] No Default No Default No Default Default    No Default No Default
## [187] No Default No Default Default    No Default Default    No Default
## [193] Default    No Default No Default No Default No Default No Default
## [199] Default    No Default No Default Default    Default    No Default
## [205] No Default No Default No Default No Default Default    No Default
## [211] No Default No Default No Default No Default Default    No Default
## [217] No Default No Default No Default Default    No Default No Default
## [223] No Default Default    No Default No Default No Default No Default
## [229] No Default No Default No Default No Default No Default No Default
## [235] No Default No Default No Default No Default No Default No Default
## [241] Default    No Default No Default Default    No Default Default   
## [247] No Default No Default No Default No Default
## Levels: Default No Default
```

---

# Test misclassification rate

To compute test misclassification


```r
miscl_k1&lt;-mean(test_y==yhat_k1)
print(miscl_k1)
```

```
## [1] 0.68
```

Overall, 68% of the training sample is correctly predicted
--

Now you obtain the misclassification rate with `\(k=5\)`

---
# Answer


```r
knn(train_x_std,
    test_x_std,
    train_y,k=5)-&gt;yhat_k5
print(yhat_k5)
```

```
##   [1] No Default Default    No Default No Default Default    No Default
##   [7] No Default No Default No Default Default    No Default No Default
##  [13] No Default No Default No Default No Default No Default No Default
##  [19] No Default No Default Default    No Default No Default No Default
##  [25] No Default No Default No Default No Default Default    Default   
##  [31] No Default No Default No Default Default    Default    No Default
##  [37] No Default No Default No Default No Default No Default No Default
##  [43] No Default No Default No Default No Default No Default No Default
##  [49] No Default No Default No Default Default    Default    No Default
##  [55] No Default Default    Default    Default    No Default Default   
##  [61] Default    No Default Default    Default    No Default No Default
##  [67] No Default No Default No Default No Default No Default No Default
##  [73] No Default No Default No Default No Default No Default No Default
##  [79] No Default No Default No Default No Default No Default No Default
##  [85] No Default No Default No Default No Default No Default No Default
##  [91] No Default No Default No Default No Default Default    No Default
##  [97] No Default No Default No Default No Default No Default No Default
## [103] No Default Default    No Default Default    No Default No Default
## [109] No Default No Default No Default No Default No Default No Default
## [115] No Default No Default No Default Default    No Default No Default
## [121] No Default No Default No Default No Default No Default No Default
## [127] No Default Default    No Default No Default No Default No Default
## [133] No Default No Default No Default No Default No Default No Default
## [139] No Default No Default No Default Default    No Default No Default
## [145] No Default No Default No Default No Default No Default No Default
## [151] No Default No Default No Default No Default No Default No Default
## [157] No Default No Default No Default No Default No Default No Default
## [163] No Default No Default Default    No Default No Default No Default
## [169] No Default No Default No Default No Default No Default No Default
## [175] Default    No Default No Default No Default Default    No Default
## [181] No Default No Default No Default No Default No Default No Default
## [187] No Default No Default No Default No Default No Default No Default
## [193] No Default No Default No Default No Default No Default No Default
## [199] No Default No Default No Default No Default Default    No Default
## [205] No Default No Default No Default No Default No Default No Default
## [211] No Default No Default No Default No Default No Default No Default
## [217] No Default No Default No Default No Default No Default No Default
## [223] No Default No Default No Default No Default No Default No Default
## [229] No Default No Default No Default No Default No Default No Default
## [235] No Default No Default Default    No Default No Default No Default
## [241] No Default No Default No Default Default    No Default No Default
## [247] No Default No Default No Default No Default
## Levels: Default No Default
```

There are also mutliple options for handling ties (for the nearest neighbours)

---

# Additional features

- If the argument `prob=TRUE` is included then the function returns probabilities.


```r
knn(train_x_std,
    test_x_std,
    train_y,k=5,prob = TRUE)-&gt;prob_k5
print(attr(prob_k5,"prob"))
```

```
##   [1] 1.0000000 0.5714286 0.6666667 0.6000000 0.6000000 0.8000000 1.0000000
##   [8] 1.0000000 0.6666667 0.6000000 0.8333333 0.8000000 1.0000000 0.5714286
##  [15] 1.0000000 0.6666667 1.0000000 1.0000000 0.6000000 1.0000000 0.6000000
##  [22] 0.8333333 0.6666667 0.6000000 1.0000000 1.0000000 0.5714286 1.0000000
##  [29] 0.6000000 0.5000000 0.6666667 0.8571429 1.0000000 0.6000000 0.6000000
##  [36] 0.8000000 0.6666667 0.6000000 0.6000000 0.5000000 0.6000000 0.6666667
##  [43] 0.6000000 0.6000000 0.8333333 1.0000000 0.6666667 0.8000000 1.0000000
##  [50] 1.0000000 0.8000000 0.6000000 0.6000000 1.0000000 1.0000000 0.6000000
##  [57] 0.6666667 0.8000000 1.0000000 0.6000000 0.8000000 1.0000000 0.6000000
##  [64] 0.6000000 0.6666667 0.8000000 0.5714286 0.5714286 1.0000000 1.0000000
##  [71] 0.6666667 0.6000000 1.0000000 0.6666667 0.8000000 0.8000000 1.0000000
##  [78] 0.8000000 0.8000000 1.0000000 0.6666667 0.6000000 1.0000000 0.8000000
##  [85] 0.5714286 1.0000000 0.8000000 0.6666667 1.0000000 0.8750000 1.0000000
##  [92] 1.0000000 0.8000000 0.7142857 0.6000000 0.7142857 1.0000000 0.8571429
##  [99] 0.8000000 0.8000000 0.8333333 0.8000000 0.5714286 0.6000000 0.8333333
## [106] 0.6000000 0.8000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
## [113] 1.0000000 0.6000000 0.8750000 0.6000000 1.0000000 0.6666667 1.0000000
## [120] 1.0000000 0.6000000 0.8333333 0.5000000 0.6000000 0.6000000 0.6666667
## [127] 1.0000000 0.6000000 0.7142857 1.0000000 0.8000000 0.8333333 1.0000000
## [134] 0.6000000 0.7142857 1.0000000 0.6000000 0.8000000 0.6666667 0.7142857
## [141] 0.6000000 0.6666667 0.8000000 0.8000000 0.8000000 1.0000000 1.0000000
## [148] 1.0000000 0.8000000 1.0000000 0.8750000 1.0000000 1.0000000 0.6000000
## [155] 1.0000000 0.8000000 0.6666667 1.0000000 0.5714286 0.8000000 0.6000000
## [162] 0.6000000 1.0000000 0.6000000 0.6000000 0.8000000 1.0000000 1.0000000
## [169] 1.0000000 0.5714286 1.0000000 0.5000000 0.8000000 0.7142857 0.6000000
## [176] 0.6666667 1.0000000 0.8750000 0.6666667 1.0000000 1.0000000 0.8000000
## [183] 0.8000000 0.6666667 0.6000000 0.8000000 0.8000000 0.8000000 0.6000000
## [190] 0.8000000 0.8000000 0.8333333 0.8000000 0.6000000 1.0000000 0.6666667
## [197] 0.8000000 0.8000000 0.8750000 0.8333333 0.7142857 0.7500000 0.6000000
## [204] 1.0000000 1.0000000 1.0000000 0.8000000 1.0000000 0.8000000 1.0000000
## [211] 0.7142857 1.0000000 1.0000000 0.6666667 0.6000000 1.0000000 1.0000000
## [218] 0.8000000 0.8333333 0.6666667 1.0000000 0.7500000 0.8000000 0.5714286
## [225] 0.8000000 1.0000000 0.8000000 1.0000000 0.8000000 1.0000000 1.0000000
## [232] 1.0000000 1.0000000 0.8333333 0.6000000 1.0000000 0.6000000 0.8000000
## [239] 1.0000000 0.6000000 0.8000000 1.0000000 1.0000000 0.6000000 0.8333333
## [246] 0.6666667 1.0000000 0.8000000 0.8000000 1.0000000
```
---

# Regression 

- The same principal can be used for regression.
--

- The predicted value is simply the average value of `\(y\)` for the `\(k\)`-nearest neighbours.
--

- This leads to a highly non-linear regression function.

Choosing `\(k=5\)` leads to better results.

---

# Conclusion

- Using k-NN is a simple but often effective method for classification.
--

- There are limitations
--

  - Complicated decision boundaries.
  - High variance.
  - Also it works poorly when there is a large number of predictors.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

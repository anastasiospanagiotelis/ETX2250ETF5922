<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Nearest Neighbour Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Anastasios Panagiotelis" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/mtheme.css" type="text/css" />
    <link rel="stylesheet" href="css/mod.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Nearest Neighbour Classification
## Data Visualisation and Analytics
### Anastasios Panagiotelis
### Lecture 9

---


class: inverse, center, middle

# Love Thy Neighbour



---

# Nearest Neighbours 

- Recall that we discussed nearest neigbours during the topic on LOESS.
--

- Now we will use nearest neighbours in the context of classification.
--

- Illustration with 2 predictors since these are easy to understand.
--

- Data is a subsample of 100 observations from the Credit default data.

---

# Credit Data

&lt;img src="kNN_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

# Default or not?

&lt;img src="kNN_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

---

# Default or not?

&lt;img src="kNN_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

# Basic Idea

- For the first point (limit 350000, age 50) most nearby points in the training data are not defaults (orange).
  - Predict no default
- For the second point (limit 50000, age 45) most nearby points in the training data are defaults (black).
  - Predict default

---

# More formally

- Concentrate on the `\(k\)` nearest points, also known as the k Nearest Neighbours or k-NN.
- Let `\(\mathcal{N}_{\mathbf{x},k}\)` be the set of `\(k\)` training points closest to `\(\mathbf{x}\)`.
- The predicted probability of Default is

`$$\frac{1}{k}\sum\limits_{i\in\mathcal{N}_{\mathbf{x},k}}I(y_i=\mbox{Default})$$`

---

# First example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

# First example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

# Prediction

- The four nearest neighbours are all *Non-defaults*
--

- Therefore the predicted default probability for an individual with limit of 350000 and age 50 is zero.
--

- We predict this individual does not default.

---


# Second example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---

# Second example (k=4)

&lt;img src="kNN_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

# Prediction

- The four nearest neighbours include three *Defaults* and one *Non-default*
--

- Therefore the predicted default probability for an individual with limit of 50000 and age 45 is 0.75.
--

- We predict this individual does default.

---

# Your Turn

- Consider the case where `\(k=10\)`
- What is the predicted probability of default for the first example (350000 limit 50 years age)?
- What is the predicted probability of default for the second example (50000 limit 45 years age)?

---

# First example (k=10)

&lt;img src="kNN_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---


# Second example (k=10)

&lt;img src="kNN_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

# Answers

- What is the predicted probability of default for the first example (350000 limit 50 years age)?
  - The predicted probability of default is 0.
- What is the predicted probability of default for the second example (50000 limit 45 years age)?
  - The predicted probability of default is 0.6.
  
---
class: inverse, middle, center

# Decision Boundary

---

# Data again

&lt;img src="kNN_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

---

# Decision Boundaries

- For the next few slides a grid of evaluation points has been created.
- If k-NN predicts "Default" the grid point is colored black.
- If k-NN predicts "No default" the grid point is colored yellow.
- Plotting these gives an idea of the *decision boundaries*.

---

# Decision boundary (k=1)

&lt;img src="kNN_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

---

# Decision boundary (k=3)

&lt;img src="kNN_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---


# Decision boundary (k=11)

&lt;img src="kNN_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

---
# Decision boundary (k=17)

&lt;img src="kNN_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---

# Choosing `\(k\)`

- Smaller values of `\(k\)` lead to very complicated decision boundaries.
- Larger values of `\(k\)` provide smoother boundaries although there are still some unstable regions.
- Larger values of `\(k\)` also slow down computation for big datasets.

---

# Choosing `\(k\)`

- A general rule of thumb is to choose `\(k\approx\sqrt{n}\)`
--

- Also it is advised to choose a value of `\(k\)` that is NOT a multiple of the number of classes.
--

- This avoids ties.
--

- When ties are present, a class is chosen randomly.
--

- For the two class problem, choose odd `\(k\)`.

---
# Choosing `\(k\)`

- The value of `\(k\)` can also be chosen by trying many possible values of `\(k\)`.
--

- The optimal value can be selected so that the *test* misclassifcation error rate is minimised.
--

- Another alternative is to use cross validation.

---
class: inverse, middle, center

# Instability of kNN

---

# High variability

- A problem of k-NN classification is that it is highly variable.
- Small changes in the training data change the decision boundaries dramatically.
- This is much more pronounced for smaller values of `\(k\)`.
- The next few slides show different subsamples of training data.

---

# Original Training Sample (k=3)

&lt;img src="kNN_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---


# Different Training Sample (k=3)

&lt;img src="kNN_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

---


# Original Training Sample (k=11)

&lt;img src="kNN_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

---


# Different Training Sample (k=11)

&lt;img src="kNN_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle

# k-NN Classification in R

---

# Package

- The k-NN algorithm can be implemented using the `class` package.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

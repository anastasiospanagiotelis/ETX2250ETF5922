---
title: "Nearest Neighbour Classification"
subtitle: "Data Visualisation and Analytics"
author: "Anastasios Panagiotelis"
date: "Lecture 9"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"css/mtheme.css","css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

class: inverse, center, middle

# Love Thy Neighbour

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
require(magrittr)
require(tidyverse)
require(plotly)
require(widgetframe)
require(animation)
require(DT)
require(PoEdata)
require(kableExtra)
require(gridExtra)
require(ggmosaic)
require(ggthemes)
```

---

# Nearest Neighbours 

- Recall that we discussed nearest neigbours during the topic on LOESS.
--

- Now we will use nearest neighbours in the context of classification.
--

- Illustration with 2 predictors since these are easy to understand.
--

- Data is a subsample of 100 observations from the Credit default data.

---

# Credit Data

```{r,echo=F,eval=T}
set.seed(1)
Credit<-readRDS('Credit.rds')%>%sample_n(100)
g<-ggplot(Credit,aes(x=LIMIT_BAL,y=AGE,col=default))+geom_point()+scale_color_colorblind()
g
```

---

# Default or not?

```{r,echo=F,eval=T}
g+geom_point(aes(x=350000,50),color='#339966',shape=4,size=10)

```

---

# Default or not?

```{r,echo=F,eval=T}
g+geom_point(aes(x=50000,45),color='#339966',shape=4,size=10)

```

---

# Basic Idea

- For the first point (limit 350000, age 50) most nearby points in the training data are not defaults (orange).
  - Predict no default
- For the second point (limit 50000, age 45) most nearby points in the training data are defaults (black).
  - Predict default

---

# More formally

- Concentrate on the $k$ nearest points, also known as the k Nearest Neighbours or k-NN.
- Let $\mathcal{N}_{\mathbf{x},k}$ be the set of $k$ training points closest to $\mathbf{x}$.
- The predicted probability of Default is

$$\frac{1}{k}\sum\limits_{i\in\mathcal{N}_{\mathbf{x},k}}I(y_i=\mbox{Default})$$

---

# First example (k=4)

```{r,echo=F,eval=T}
g+geom_point(aes(x=350000,50),color='#339966',shape=4,size=10)

```

---

# First example (k=4)

```{r,echo=F,eval=T}
g+geom_point(aes(x=350000,50),color='#339966',shape=4,size=10)+geom_point(aes(x=350000,50),color='#339966',shape=1,size=45)

```

---

# Prediction

- The four nearest neighbours are all *Non-defaults*
--

- Therefore the predicted default probability for an individual with limit of 350000 and age 50 is zero.
--

- We predict this individual does not default.

---


# Second example (k=4)

```{r,echo=F,eval=T}
g+geom_point(aes(x=50000,45),color='#339966',shape=4,size=10)

```

---

# Second example (k=4)

```{r,echo=F,eval=T}
g+geom_point(aes(x=50000,45),color='#339966',shape=4,size=10)+geom_point(aes(x=50000,45),color='#339966',shape=1,size=25)

```

---

# Prediction

- The four nearest neighbours include three *Defaults* and one *Non-default*
--

- Therefore the predicted default probability for an individual with limit of 50000 and age 45 is 0.75.
--

- Using a threshold of 0.5, we predict this individual does default.
--

- In practice a different threshold can be used.

---

# Your Turn

- Consider the case where $k=10$
- What is the predicted probability of default for the first example (350000 limit 50 years age)?
- What is the predicted probability of default for the second example (50000 limit 45 years age)?

---

# First example (k=10)

```{r,echo=F,eval=T}
g+geom_point(aes(x=350000,50),color='#339966',shape=4,size=10)+geom_point(aes(x=350000,50),color='#339966',shape=1,size=110)

```

---


# Second example (k=10)

```{r,echo=F,eval=T}
g+geom_point(aes(x=50000,45),color='#339966',shape=4,size=10)+geom_point(aes(x=50000,45),color='#339966',shape=1,size=45)

```

---

# Answers

- What is the predicted probability of default for the first example (350000 limit 50 years age)?
  - The predicted probability of default is 0.
- What is the predicted probability of default for the second example (50000 limit 45 years age)?
  - The predicted probability of default is 0.6.
  
---
class: inverse, middle, center

# Decision Boundary

---

# Data again

```{r}

grid<-Credit[,c(2,6)]
colnames(grid)<-c('Limit','Age')
grid%>%
  as_tibble()%>%
  add_column(Class=as.factor(pull(Credit,25)))%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point()+scale_color_colorblind()
```

---

# Decision Boundaries

- For the next few slides a grid of evaluation points has been created.
- If k-NN predicts "Default" the grid point is colored black.
- If k-NN predicts "No default" the grid point is colored yellow.
- Plotting these gives an idea of the *decision boundaries*.

---

# Decision boundary (k=1)

```{r, echo=FALSE,eval=TRUE}
library(class)
gx1<-seq(10000,500000, by=10000)
gx2<-seq(20.5,60, by=0.5)
grid<-expand.grid(gx1,gx2)
colnames(grid)<-c('Limit','Age')
#grid<-cbind(grid,rbinom(nrow(grid),1,0.5))
select(Credit,LIMIT_BAL,AGE)%>%scale->tr
ts<-scale(grid,center = attr(tr,"scaled:center") ,scale = attr(tr,"scaled:scale") )

knn(tr, ts,cl = as.factor(pull(Credit,25)),k = 1)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---

# Decision boundary (k=3)

```{r, echo=FALSE,eval=TRUE}
knn(tr, ts,cl = as.factor(pull(Credit,25)),k = 3)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---


# Decision boundary (k=11)

```{r, echo=FALSE,eval=TRUE}
knn(tr, ts,cl = as.factor(pull(Credit,25)),k = 11)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---
# Decision boundary (k=17)

```{r, echo=FALSE,eval=TRUE}
knn(tr, ts,cl = as.factor(pull(Credit,25)),k = 17)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---

# Choosing $k$

- Smaller values of $k$ lead to very complicated decision boundaries.
- Larger values of $k$ provide smoother boundaries although there are still some unstable regions.
- Larger values of $k$ also slow down computation for big datasets.

---

# Choosing $k$

- A general rule of thumb is to choose $k\approx\sqrt{n}$
--

- Also it is advised to choose a value of $k$ that is NOT a multiple of the number of classes.
--

- This avoids ties.
--

- When ties are present, a class is chosen randomly.
--

- For the two class problem, choose odd $k$.

---
# Choosing $k$

- Finally, data can be split into a training sample and test sample.
--

- We can then try many possible values of $k$.
--

- The optimal value can be selected so that the *test* misclassification error rate is minimised.
--

- Another alternative is to use cross validation.

---
class: inverse, middle, center

# Instability of kNN

---

# High variability

- A problem of k-NN classification is that it is highly variable.
--

- Now assume we use a fixed value of $k$ but make small changes in the training data
--

- This change the decision boundaries dramatically.
--

- This is much more pronounced for smaller values of $k$.
--

- The next few slides show different subsamples of training data.

---

# Original Training Sample (k=3)

```{r, echo=FALSE,eval=TRUE}
knn(tr, ts,cl = as.factor(pull(Credit,25)),k = 3)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---


# Different Training Sample (k=3)

```{r, echo=FALSE,eval=TRUE}
set.seed(2)
Credit2<-readRDS('Credit.rds')%>%sample_n(100)
select(Credit2,LIMIT_BAL,AGE)%>%scale->tr2
ts2<-scale(grid,center = attr(tr2,"scaled:center") ,scale = attr(tr2,"scaled:scale") )
knn(tr2, ts2,cl = as.factor(pull(Credit2,25)),k = 3)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---


# Original Training Sample (k=11)

```{r, echo=FALSE,eval=TRUE}
knn(tr, ts,cl = as.factor(pull(Credit,25)),k = 11)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---


# Different Training Sample (k=11)

```{r, echo=FALSE,eval=TRUE}
knn(tr2, ts2,cl = as.factor(pull(Credit2,25)),k = 11)->out

grid%>%
  as_tibble%>%
  add_column(Class=out)%>%
  ggplot(aes(x=Limit,y=Age,col=Class))+geom_point(size=1)+scale_color_colorblind()

```

---

# An ethical problem

- A classifier with such high variability obviously poses statistical problems.
--

- It also poses ethical issues.
--

- Suppose the algorithm is used to decide who recieves a loan or who is audited by the tax office.
--

- It should be possible to clearly explain to this person why their loan application was rejected or why they were chosen for audit.
--

- Having such a sensitive algorithm makes this difficult.

---
class: inverse, center, middle

# k-NN Classification in R

---

# Package

- The k-NN algorithm can be implemented using the `class` package.
--

- We will go through an example using the data that has already been cleaned and is in the file *knnexample.RData*
--

- Load this file using
```{r,echo=T,eval=T}
load('knnexample.RData')
```

---

# Data

- A data frame of 250 randomly selected training observations for the predictors LIMIT_BAL and AGE (`train_x`)
--

- The values of the target variable for the training data stored as a factor (`train_y`)
--

- A data frame of 250 randomly selected test observations for the predictors LIMIT_BAL and AGE (`test_x`)
--

- The values of the target variable for the test data stored as a factor (`test_y`)

---

# Standardisation

- An important issue with k-NN is that distances can be sensitive to units of measurement.
--

- For this reason we should standardise so that all predictors have a standard deviation of 1. 

```{r, echo=T, eval=T}

train_x_std<-scale(train_x)
mean_train_x<-attr(train_x_std,"scaled:center")
std_train_x<-attr(train_x_std,"scaled:scale")

```

---

# Standardise training data

- The test data should also be standardised but using the same mean and standard deviation

```{r,echo=T, evel=T}
test_x_std<-scale(test_x,
                   center = mean_train_x,
                   scale = std_train_x)

```

- Now the data are ready to be used in the `knn` function

---

# Using knn

With k=1

```{r, eval=T,echo=T}

knn(train_x_std,test_x_std,
    train_y,k=1)->yhat_k1
print(yhat_k1)

```

---

# Test misclassification rate

To compute test misclassification

```{r,echo=T}

miscl_k1<-mean(test_y!=yhat_k1)
print(miscl_k1)

```

Overall, 32% of the training sample is incorrectly predicted
--

- As an exercise, you can now try to obtain the misclassification rate with $k=5$

---
# Answer

```{r, eval=T,echo=T}


knn(train_x_std,
    test_x_std,
    train_y,k=5)->yhat_k5
miscl_k5<-mean(test_y!=yhat_k5)
print(miscl_k5)

```

The misclassification rate when $k=5$ is `r round(miscl_k5,3)`.

---

#Sensitivity and specificity

- It is also worth reporting results in a cross tabulation

```{r,echo=T,eval=T}
table(train_y,yhat_k5)
```

Letting "default" be the "positive" class, sensitivity is 9/(9+53) or `r round(9/(9+53),3)`, and specificity is 168/(20+168) or `r round(168/(20+168),3)`

---

# Additional features

- If the argument `prob=TRUE` is included then the function returns probabilities.

```{r, eval=T,echo=T}

knn(train_x_std,
    test_x_std,
    train_y,k=5,prob = TRUE)->prob_k5
print(attr(prob_k5,"prob"))

```

---


# Ties in neighbours

- If there are only 5 neighbours all probabilities should be either 0, 0.2, 0.4, 0.6, 0.8 or 1.
--

- This is not true.  Why?
--

- There can be ties in finding the nearest neighbours.
--

- Two (or more) observations may tied as equally fifth closest.
--

- In this case both are used.

---

# Regression 

- The same principle can be used for regression.
--

- The predicted value is simply the average value of $y$ for the $k$-nearest neighbours.
--

- This leads to a highly non-linear regression function.
---


# Conclusion

- Using k-NN is a simple but often effective method for classification.
--

- There are limitations
--

  - Complicated decision boundaries.
  - High variance.
  - Also it works poorly when there is a large number of predictors.